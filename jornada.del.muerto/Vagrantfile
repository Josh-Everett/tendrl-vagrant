# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'yaml'

# Global Variables
VAGRANT_ROOT = File.dirname(File.expand_path(__FILE__))

ENV['VAGRANT_DEFAULT_PROVIDER'] = 'libvirt'

# Disable parallel runs - breaks peer probe in the end
ENV['VAGRANT_NO_PARALLEL'] = 'yes'

#################
# General VM settings applied to all VMs
#################
VMCPU = 1         # number of cores per VM
VMMEM = 1024      # amount of memory in MB per VM
VMDISK = '256m'.freeze # size of brick disks in MB
# Metadata volume isn't generated if <200MB:
# https://github.com/gluster/gdeploy/blob/0462ad54f1d8f9c83502e774246a528ae2c8c83f/modules/lv.py#L168

#################
# Config Loading
#################

zombie_count = -1

# load the YAML file then munge
tendrl_conf = YAML.load_file 'zombies.conf.yml'
zombie_count = tendrl_conf['zombie_count'].to_i

#################

def vb_attach_disks(disks, provider, boxName)
  (1..disks).each do |i|
    # File.join joins the given parameters with '/'
    file_to_disk = File.join VAGRANT_ROOT, 'disks', "#{boxName}-disk#{i}.vdi"
    unless File.exist?(file_to_disk)
      provider.customize [
        'createhd',
        '--filename', file_to_disk,
        '--size', VMDISK.to_i
      ]
    end
    provider.customize [
      'storageattach',
      :id,
      '--storagectl', 'SATA Controller',
      '--port', i,
      '--device', 0,
      '--type', 'hdd',
      '--medium', file_to_disk
    ]
  end
end

def libvirt_attach_disks(disks, provider)
  (1..disks).each do
    provider.storage :file, bus: 'virtio', size: VMDISK
  end
end





Vagrant.configure(2) do |config|
  config.vm.box = 'centos/7'
  
  config.vm.provider 'libvirt' do |libvirt, _override|
    # Use virtio device drivers
    libvirt.nic_model_type = 'virtio'
    libvirt.disk_bus = 'virtio'
    libvirt.storage_pool_name = ENV['LIBVIRT_STORAGE_POOL'] || 'default'
  end

  config.vm.network 'private_network', type: 'dhcp', auto_config: true

  # https://www.vagrantup.com/docs/synced-folders/basic_usage.html
  # first parameter is a path to a directory on the host machine.
  # second parameter must be an absolute path of where
  # to share the folder within the guest machine. 
  config.vm.synced_folder 'api', '/usr/share/tendrl-api', disabled: true,
    type: 'rsync', rsync__exclude: %w[.git vendor/bundle .bundle .gitignore .rspec .ruby-gemset .ruby-version .travis.yml],
    rsync__args: ["--verbose", "--rsync-path='sudo rsync'", "--archive", "-z"]
  
  (1..zombie_count).each do |node_index|
    config.vm.define "zombie-#{node_index}" do |machine|
      # Provider-independent options
      machine.vm.hostname = "tendrl-node-#{node_index}"
      machine.vm.synced_folder '.', '/vagrant', disabled: true

      machine.vm.provider 'libvirt' do |libvirt, override|
        # Set VM resources
        libvirt.memory = VMMEM
        libvirt.cpus = VMCPU

        # Use virtio device drivers
        libvirt.nic_model_type = 'virtio'
        libvirt.disk_bus = 'virtio'

        # connect to local libvirt daemon as root
        libvirt.username = 'root'

        # attach brick disks
        libvirt_attach_disks(disk_count, libvirt)
      end


      # https://www.vagrantup.com/docs/provisioning/ansible_common.html
      if node_index == storage_node_count
        # Last node begins execution of our ansible files
        # 1) prepare environment
        # 2) prepare gluster
        # 3) deploy cluster
        # 4) tendrl-site / tendril_init
        # 5) update-tendrl
        
        machine.vm.provision :prepare_env, type: :ansible do |ansible|
           # Setting limit = "all" can be used to make Ansible connect to all machines from the inventory file.
          ansible.limit = 'all'
          
          ansible.groups = {
            'gluster-servers' => ["tendrl-node-[1:#{storage_node_count}]"],
            'tendrl-server' => ['tendrl-server']
          }
          ansible.playbook = 'ansible/prepare-environment.yml'
        end

        machine.vm.provision :prepare_gluster, type: :ansible do |ansible|
          ansible.limit = 'all'
          ansible.groups = {
            'gluster-servers' => ["tendrl-node-[1:#{storage_node_count}]"]
          }
          ansible.playbook = 'ansible/prepare-gluster.yml'
        end

        if cluster_init
          machine.vm.provision :deploy_cluster, type: :ansible do |ansible|
            ansible.limit = 'all'
            ansible.playbook = 'ansible/deploy-cluster.yml'
            ansible.groups = {
              'gluster-servers' => ["tendrl-node-[1:#{storage_node_count}]"]
            }
            ansible.extra_vars = {
              provider: ENV['VAGRANT_DEFAULT_PROVIDER'],
              storage_node_count: storage_node_count
            }
          end
        end

        if tendrl_init
          ENV['ANSIBLE_ROLES_PATH'] = "#{ENV['ANSIBLE_ROLES_PATH']}:tendrl-ansible/roles"
          puts '  Pulling submodule Tendrl/tendrl-ansible'
          `git submodule init`
          `git submodule update`
          machine.vm.provision :deploy_tendrl, type: :ansible do |ansible|
            ansible.limit = 'all'
            ansible.groups = {
              'gluster-servers' => ["tendrl-node-[1:#{storage_node_count}]"],
              'tendrl-server' => ['tendrl-server']
            }
            ansible.playbook = 'ansible/tendrl-site.yml'
          end

          machine.vm.provision :update_tendrl, type: :ansible do |ansible|
            ansible.limit = 'all'
            ansible.groups = {
              'gluster-servers' => ["tendrl-node-[1:#{storage_node_count}]"],
              'tendrl-server' => ['tendrl-server']
            }
            ansible.playbook = 'ansible/update-tendrl.yml'
          end
        end
      end
    end
  end
end
